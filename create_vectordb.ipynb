{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "import openai\n",
    "import docx2txt\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import UnstructuredPowerPointLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500, chunk_overlap = 200)\n",
    "\n",
    "def get_files_with_extension(folder_path, file_extension):\n",
    "\n",
    "    file_list = []\n",
    "\n",
    "    for elem in folder_path:\n",
    "        for root, _, files in os.walk(elem):\n",
    "            for file in files:\n",
    "                if file.endswith(file_extension):    \n",
    "                    # get the full path and file size\n",
    "                    file_path = os.path.join(root, file).replace(\"\\\\\", \"/\")\n",
    "                    file_list.append(file_path)\n",
    "\n",
    "    print(f\"Number of founded documents: {len(file_list)}\")\n",
    "    \n",
    "    return file_list\n",
    "\n",
    "def add_context_to_doc_chunks(_docs):\n",
    "\n",
    "    # adding the filename to each chunk my help the relevany search\n",
    "\n",
    "    for i in _docs:\n",
    "        i.page_content = ' '.join(i.metadata['source'].split(\"\\\\\")[-1].split('.')[:-1]) + ' \\n\\n' + i.page_content\n",
    "\n",
    "    return _docs\n",
    "\n",
    "def create_db(_docs,_embeddings = None):\n",
    "\n",
    "    if not _embeddings:\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "    else:\n",
    "        embeddings = _embeddings\n",
    "    \n",
    "    db = FAISS.from_documents(_docs, embeddings)\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    response = openai.Embedding.create(input=text, model = 'text-embedding-ada-002')\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "def load_pdf(pdf_as_bytes, splitter = text_splitter, filename = 'pdf'):\n",
    "\n",
    "    pdf_as_bytes = PdfReader(pdf_as_bytes)\n",
    "\n",
    "    #text = ''\n",
    "    DOCS = []\n",
    "\n",
    "    for pagenum, page in enumerate(pdf_as_bytes.pages):\n",
    "\n",
    "        page_text = page.extract_text()\n",
    "\n",
    "        text_splitted = splitter.split_text(page_text)\n",
    "        docs = [Document(page_content=t, metadata={'source' : filename, 'page' : str(pagenum+1)}) for t in text_splitted]\n",
    "        docs = add_context_to_doc_chunks(docs)\n",
    "        \n",
    "        DOCS.append(docs)\n",
    "\n",
    "    DOCS = [item for sublist in DOCS for item in sublist]\n",
    "\n",
    "    return DOCS\n",
    "\n",
    "def load_docx(file, splitter = text_splitter, filename = 'docx'):\n",
    "\n",
    "    DOCS = []\n",
    "\n",
    "    text = docx2txt.process(file) \n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "\n",
    "    text_splitted = splitter.split_text(text)\n",
    "    docs = [Document(page_content=t, metadata={'source' : filename, 'page' : 'all'}) for t in text_splitted]\n",
    "    docs = add_context_to_doc_chunks(docs)\n",
    "    DOCS.append(docs)\n",
    "\n",
    "    DOCS = [item for sublist in DOCS for item in sublist]\n",
    "\n",
    "    return DOCS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='intfloat/multilingual-e5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname_list = [\"data\"]\n",
    "\n",
    "# giving file extension\n",
    "ext = ('.pdf','.docx')\n",
    "filtered_files = get_files_with_extension(dirname_list, ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_all = []\n",
    "for file in filtered_files:\n",
    "    filename = file.split('/')[-1]\n",
    "    print(filename)\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_doc_chunks = load_pdf(file, filename = filename)\n",
    "        docs_all.extend(pdf_doc_chunks)\n",
    "    if file.endswith('.docx'):\n",
    "        docx_doc_chunks = load_docx(file, filename = filename)\n",
    "        docs_all.extend(docx_doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, db = create_db(docs_all, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db.save_local(\"faiss_index_e5_large_pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_with_extension(folder_path, file_extension):\n",
    "\n",
    "    file_list = []\n",
    "\n",
    "    for elem in folder_path:\n",
    "        for root, _, files in os.walk(elem):\n",
    "            for file in files:\n",
    "                if file.endswith(file_extension):    \n",
    "                    # get the full path and file size\n",
    "                    file_path = os.path.join(root, file).replace(\"\\\\\", \"/\")\n",
    "                    file_list.append(file_path)\n",
    "\n",
    "    print(f\"Number of founded documents: {len(file_list)}\")\n",
    "    \n",
    "    return file_list\n",
    "def add_context_to_doc_chunks(_docs):\n",
    "\n",
    "    # adding the ppt page header to each chunk\n",
    "\n",
    "    if _docs[0].metadata['source'].split('.')[0] == 'MPTHK_202212':\n",
    "        for i in _docs:\n",
    "            i.page_content = 'Magyar Posta Takarék Hosszú Kötvény Befektetési Alap' + ' \\n\\n' + i.page_content\n",
    "    if _docs[0].metadata['source'].split('.')[0] == 'MPTHV_202212':\n",
    "        for i in _docs:\n",
    "            i.page_content = 'Magyar Posta Takarék Harmónia Vegyes Befektetési Alap' + ' \\n\\n' + i.page_content\n",
    "    if _docs[0].metadata['source'].split('.')[0] == 'OPTII_202212':\n",
    "        for i in _docs:\n",
    "            i.page_content = 'DIÓFA Optimus III. Befektetési Alap „A” sorozat' + ' \\n\\n' + i.page_content\n",
    "    if _docs[0].metadata['source'].split('.')[0] == 'OPTI_202212':\n",
    "        for i in _docs:\n",
    "            i.page_content = 'DIÓFA Optimus I. Befektetési Alap „A” sorozat' + ' \\n\\n' + i.page_content\n",
    "    if _docs[0].metadata['source'].split('.')[0] == 'TAHB_202212':\n",
    "        for i in _docs:\n",
    "            i.page_content = 'Takarék Abszolút Hozamú Befektetési Alap' + ' \\n\\n' + i.page_content\n",
    "    if _docs[0].metadata['source'].split('.')[0] == 'Tapollo_202212':\n",
    "        for i in _docs:\n",
    "            i.page_content = 'Takarék Apollo Származtatott Részvény Befektetési Alap' + ' \\n\\n' + i.page_content\n",
    "    if _docs[0].metadata['source'].split('.')[0] == 'TSZ_202212':\n",
    "        for i in _docs:\n",
    "            i.page_content = 'Takarék Származtatott Befektetési Alap' + ' \\n\\n' + i.page_content\n",
    "    else:\n",
    "        for i in _docs:\n",
    "            i.page_content = _docs[0].page_content.split('\\n\\n')[0] + ' \\n\\n' + i.page_content\n",
    "\n",
    "    # adding the filename to each chunk my help the relevany search\n",
    "\n",
    "    for i in _docs:\n",
    "        i.page_content = ' '.join(i.metadata['source'].split(\"\\\\\")[-1].split('.')[:-1]) + ' \\n\\n' + i.page_content\n",
    "\n",
    "\n",
    "    return _docs\n",
    "def load_pptx(file_path, splitter = text_splitter, filename = 'pptx'):\n",
    "\n",
    "    DOCS = []\n",
    "\n",
    "    #text = UnstructuredPowerPointLoader(file_path).load()\n",
    "    text_splitted = UnstructuredPowerPointLoader(file_path).load_and_split(text_splitter=splitter)\n",
    "    \n",
    "\n",
    "    #docs = [Document(page_content=text[0].page_content, metadata={'source' : filename, 'page' : 'all'})]\n",
    "    docs = [Document(page_content=t.page_content, metadata={'source' : filename, 'page' : 'all'}) for t in text_splitted]\n",
    "    docs = add_context_to_doc_chunks(docs)\n",
    "    DOCS.append(docs)\n",
    "\n",
    "    DOCS = [item for sublist in DOCS for item in sublist]\n",
    "\n",
    "    return DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname_list = [\"data\"]\n",
    "\n",
    "# giving file extension\n",
    "ext = ('.pptx')\n",
    "filtered_files = get_files_with_extension(dirname_list, ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for file in filtered_files:\n",
    "    filename = file.split('/')[-1]\n",
    "    pptx_doc_chunks = load_pptx(file,splitter=text_splitter,  filename = filename)\n",
    "    docs.extend(pptx_doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_db = FAISS.from_documents(docs, embeddings)\n",
    "#ppt_db.save_local(\"faiss_index_pptx500_context_cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.merge_from(ppt_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db.save_local(\"faiss_index_e5_large_pre\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diofagpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
